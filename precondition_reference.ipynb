{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read training, dev and unlabeled test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following provides a starting code (Python 3) of how to read the labeled training and dev sentence pairs, and unlabeled test sentence pairs, into lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, dev, test = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5983\n",
      "[['Sometimes do exercise.', 'A person typically desire healthy life.', '1'], ['Who eats junk foods.', 'A person typically desire healthy life.', '0'], ['A person is sick.', 'A person typically desire healthy life.', '1']]\n"
     ]
    }
   ],
   "source": [
    "with open('./data/pnli_train.csv', encoding='utf-8') as fp:\n",
    "    csvreader = csv.reader(fp)\n",
    "    for x in csvreader:\n",
    "        # x[2] will be the label (0 or 1). x[0] and x[1] will be the sentence pairs.\n",
    "        train.append(x)\n",
    "print (len(train))\n",
    "print (train[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1055\n",
      "[['A person is looking for accuracy.', 'A person typically desires accurate results.', '1'], ['A person does not care for accuracy.', 'A person typically desires accurate results.', '0'], ['The person double checks their data.', 'A person typically desires accurate results.', '1']]\n"
     ]
    }
   ],
   "source": [
    "with open('./data/pnli_dev.csv', encoding='utf-8') as fp:\n",
    "    csvreader = csv.reader(fp)\n",
    "    for x in csvreader:\n",
    "        # x[2] will be the label (0 or 1). x[0] and x[1] will be the sentence pairs.\n",
    "        dev.append(x)\n",
    "print (len(dev))\n",
    "print (dev[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4850\n",
      "[['The people want to have a romantic and pleasant feel.', 'People typically does desire to smell violets.'], ['The contract is to buy products from you.', 'Getting contract typically cause to make money or spend money.'], ['Train station is closed.', 'Line can typically be used to move train along tracks.']]\n"
     ]
    }
   ],
   "source": [
    "with open('./data/pnli_test_unlabeled.csv', encoding='utf-8') as fp:\n",
    "    csvreader = csv.reader(fp)\n",
    "    for x in csvreader:\n",
    "        # x[0] and x[1] will be the sentence pairs.\n",
    "        test.append(x)\n",
    "print (len(test))\n",
    "print (test[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Code Body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may choose to experiment with different methods using your program. However, you need to embed the training and inference processes at here. We will use your prediction on the unlabeled test data to grade, while checking this part to understand how your method has produced the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "import transformers\n",
    "from transformers import BertTokenizer, BertForSequenceClassification,RobertaTokenizer, RobertaForSequenceClassification,AdamW\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import random\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5983, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sometimes do exercise.</td>\n",
       "      <td>A person typically desire healthy life.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who eats junk foods.</td>\n",
       "      <td>A person typically desire healthy life.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A person is sick.</td>\n",
       "      <td>A person typically desire healthy life.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A person is dead.</td>\n",
       "      <td>A person typically desire healthy life.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A person eats properly and do exercise regularly.</td>\n",
       "      <td>A person typically desire healthy life.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             premise  \\\n",
       "0                             Sometimes do exercise.   \n",
       "1                               Who eats junk foods.   \n",
       "2                                  A person is sick.   \n",
       "3                                  A person is dead.   \n",
       "4  A person eats properly and do exercise regularly.   \n",
       "\n",
       "                                hypothesis  label  \n",
       "0  A person typically desire healthy life.      1  \n",
       "1  A person typically desire healthy life.      0  \n",
       "2  A person typically desire healthy life.      1  \n",
       "3  A person typically desire healthy life.      0  \n",
       "4  A person typically desire healthy life.      1  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.DataFrame (train, columns = [\"premise\", \"hypothesis\",\"label\"])\n",
    "print(df_train.shape)\n",
    "df_train['label'] = df_train['label'].astype(int)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1055, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A person is looking for accuracy.</td>\n",
       "      <td>A person typically desires accurate results.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A person does not care for accuracy.</td>\n",
       "      <td>A person typically desires accurate results.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The person double checks their data.</td>\n",
       "      <td>A person typically desires accurate results.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The person speeds through the experiment.</td>\n",
       "      <td>A person typically desires accurate results.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A person is studying well.</td>\n",
       "      <td>A person typically desires accurate results.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     premise  \\\n",
       "0          A person is looking for accuracy.   \n",
       "1       A person does not care for accuracy.   \n",
       "2       The person double checks their data.   \n",
       "3  The person speeds through the experiment.   \n",
       "4                 A person is studying well.   \n",
       "\n",
       "                                     hypothesis  label  \n",
       "0  A person typically desires accurate results.      1  \n",
       "1  A person typically desires accurate results.      0  \n",
       "2  A person typically desires accurate results.      1  \n",
       "3  A person typically desires accurate results.      0  \n",
       "4  A person typically desires accurate results.      1  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev = pd.DataFrame (dev, columns = [\"premise\", \"hypothesis\",\"label\"])\n",
    "df_dev['label'] = df_dev['label'].astype(int)\n",
    "print(df_dev.shape)\n",
    "df_dev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4850, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The people want to have a romantic and pleasan...</td>\n",
       "      <td>People typically does desire to smell violets.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The contract is to buy products from you.</td>\n",
       "      <td>Getting contract typically cause to make money...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Train station is closed.</td>\n",
       "      <td>Line can typically be used to move train along...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>There is no water for driving the boats.</td>\n",
       "      <td>People typically desires drive boats for fun.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The poet is busy.</td>\n",
       "      <td>Poet can typically be used for creating poetry.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             premise  \\\n",
       "0  The people want to have a romantic and pleasan...   \n",
       "1          The contract is to buy products from you.   \n",
       "2                           Train station is closed.   \n",
       "3           There is no water for driving the boats.   \n",
       "4                                  The poet is busy.   \n",
       "\n",
       "                                          hypothesis  \n",
       "0     People typically does desire to smell violets.  \n",
       "1  Getting contract typically cause to make money...  \n",
       "2  Line can typically be used to move train along...  \n",
       "3      People typically desires drive boats for fun.  \n",
       "4    Poet can typically be used for creating poetry.  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.DataFrame(test, columns = [\"premise\", \"hypothesis\"])\n",
    "print(df_test.shape)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    3145\n",
       "0    2838\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set_theme(style=\"darkgrid\")\n",
    "sns.countplot(x=\"label\", data=df_train, palette=\"rocket\")\n",
    "plt.xlabel('Labels)')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6909eb0e843148bb86af3a6c344b3be6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e55a0afb16d8470c9faf2ca892dbfeee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aca4185b06540c48e438c3d04b892b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.dataframe = df\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    def __getitem__(self, index):\n",
    "        tokenization_values = roberta_tokenizer.encode_plus(\n",
    "                    self.dataframe.loc[index, 'premise'],  self.dataframe.loc[index, 'hypothesis'],  \n",
    "                    max_length = 128,   \n",
    "                    truncation=True,\n",
    "                    return_attention_mask = True,  \n",
    "                    pad_to_max_length = True, \n",
    "                    return_tensors = 'pt',         \n",
    "               )  \n",
    "        return (tokenization_values['input_ids'][0], tokenization_values['attention_mask'][0], torch.tensor(self.dataframe.loc[index, 'label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1c063b42b5f4d71bc1c6fb6f9f86366",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/478M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuda_cpu = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "roberta_model = RobertaForSequenceClassification.from_pretrained('roberta-base',num_labels = 2)\n",
    "roberta_model.to(cuda_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_optimizer = AdamW(roberta_model.parameters(),lr = 2e-5, eps = 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = TrainingDataset(df_train)\n",
    "train_dataloader = torch.utils.data.DataLoader(training_dataset, batch_size=32,shuffle=True, num_workers=os.cpu_count())\n",
    "development_dataset = TrainingDataset(df_dev)\n",
    "val_dataloader = torch.utils.data.DataLoader(development_dataset,batch_size=32,shuffle=True, num_workers=os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of Epoch 1 / 4\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/ayushtripathi/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/Users/ayushtripathi/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'TrainingDataset' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/6f/yp95ybt12mdbhlpqx6rscjbh0000gn/T/ipykernel_76272/2610419665.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mroberta_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mroberta_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         training_result = roberta_model(input_ids.to(cuda_cpu), \n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    366\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_worker_number_rationality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_MultiProcessingDataLoaderIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0;31m#     before it starts, and __del__ tries to join but will get:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m             \u001b[0;31m#     AssertionError: can only join a started process.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m             \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index_queues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_queue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/multiprocessing/process.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    119\u001b[0m                \u001b[0;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mDefaultContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpopen_spawn_posix\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0;32mclass\u001b[0m \u001b[0mForkServerProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseProcess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mduplicate_for_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinalizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_launch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mduplicate_for_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\u001b[0m in \u001b[0;36m_launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentinel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparent_r\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosefd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                 \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetbuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mfds_to_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(0, 4): \n",
    "    print(' Number of Epoch {:} / {:}'.format(epoch + 1, 4))\n",
    "    \n",
    "\n",
    "    list_of_targets,list_of_targets_value = [], [] \n",
    "\n",
    "    training_loss_roberta = 0\n",
    "    roberta_model.train()\n",
    "    torch.set_grad_enabled(True)\n",
    "    for i, (input_ids, input_mask, labels) in enumerate(train_dataloader):\n",
    "        roberta_model.zero_grad()        \n",
    "        training_result = roberta_model(input_ids.to(cuda_cpu), \n",
    "                    attention_mask=input_mask.to(cuda_cpu),\n",
    "                    labels=labels.to(cuda_cpu))\n",
    "        loss = training_result[0]\n",
    "        training_loss_roberta = training_loss_roberta + loss.item()\n",
    "        training_predictions = training_result[1].detach().cpu().numpy()\n",
    "        targets_np = labels.to('cpu').numpy()\n",
    "        list_of_targets.extend(targets_np)\n",
    "        if i == 0:  \n",
    "            vstack_training_predictions = training_predictions\n",
    "        else:\n",
    "            vstack_training_predictions = np.vstack((vstack_training_predictions, training_predictions))\n",
    "        adam_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(roberta_model.parameters(), 1.0)\n",
    "        adam_optimizer.step() \n",
    "    y_true_train = list_of_targets\n",
    "    y_pred_train = np.argmax(vstack_training_predictions, axis=1)\n",
    "    train_acc = accuracy_score(y_true_train, y_pred_train)\n",
    "\n",
    "\n",
    "    print('Training Loss ===>' ,training_loss_roberta)\n",
    "    print(\"Training Accuracy ===>\",train_acc)\n",
    "    print('\\n')\n",
    "    print(\"Validation Section Starts Here\")\n",
    "\n",
    "\n",
    "\n",
    "    roberta_model.eval()\n",
    "    torch.set_grad_enabled(False)\n",
    "    validation_loss = 0\n",
    "    \n",
    "    \n",
    "    for j, (input_ids, input_mask, labels) in enumerate(tqdm(val_dataloader)):\n",
    "        b_input_ids = input_ids.to(cuda_cpu)\n",
    "        b_input_mask = input_mask.to(cuda_cpu)\n",
    "        b_labels = labels.to(cuda_cpu)      \n",
    "        validation_result = roberta_model(b_input_ids, \n",
    "                attention_mask=b_input_mask, \n",
    "                labels=b_labels)\n",
    "        \n",
    "        loss = validation_result[0]\n",
    "        validation_loss = validation_loss + loss.item()\n",
    "        preds = validation_result[1]\n",
    "\n",
    "        #prediction_values = preds.detach().cpu().numpy()\n",
    "\n",
    "        #targets_np = b_labels.to('cpu').numpy()\n",
    "        list_of_targets_value.extend(b_labels.to('cpu').numpy())\n",
    "\n",
    "        if j == 0:  \n",
    "            #vstack_validation_predictions = prediction_values\n",
    "            vstack_validation_predictions = preds.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "        else:\n",
    "            #vstack_validation_predictions = np.vstack((vstack_validation_predictions, prediction_values))\n",
    "            vstack_validation_predictions = np.vstack((vstack_validation_predictions, preds.detach().cpu().numpy()))\n",
    "\n",
    "\n",
    "    true_validation_accuracy = list_of_targets_value\n",
    "\n",
    "    prediction_validation_accuracy = np.argmax(vstack_validation_predictions, axis=1)\n",
    "    validation_accuracy = accuracy_score(true_validation_accuracy, prediction_validation_accuracy)\n",
    "\n",
    "    print('Validation Loss ===> ' ,validation_loss)\n",
    "    print('Validation Accuracy ===>  ', validation_accuracy)\n",
    "\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Prediction Result File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to submit a prediction result file. It should have 2028 lines, every line should be either 0 or 1, which is your model's prediction on the respective test set instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppose you had your model's predictions on the 2028 test cases read from test_enc_unlabeled.tsv, and \n",
    "#those results are in the list called 'results'\n",
    "assert (len(results) == 4850)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the results are not float numbers, but intergers 0 and 1\n",
    "results = [int(x) for x in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your prediction results to 'upload_predictions.txt' and upload that later\n",
    "with open('upload_predictions.txt', 'w', encoding = 'utf-8') as fp:\n",
    "    for x in results:\n",
    "        fp.write(str(x) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
